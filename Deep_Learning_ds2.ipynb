{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL3L8hHjNRXw",
        "outputId": "600ce878-5045-459c-ac55-66a65c3ad1ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8TgaaTnNonq",
        "outputId": "5738c2fc-c0e6-42a0-e30a-3bc0ac57785c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_0.model  model_4.model  train\t\t  train_history3.png\n",
            "model_1.model  model_5.model  train_history0.png  train_history4.png\n",
            "model_2.model  run1\t      train_history1.png  train_history5.png\n",
            "model_3.model  test\t      train_history2.png  validation\n"
          ]
        }
      ],
      "source": [
        "#import os\n",
        "#os.chdir(\"/content/drive/MyDrive/DL_Project/dataset2\")\n",
        "#!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrn1-wDfIC_H"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "dataset = 'https://drive.google.com/uc?id=1wV6Sx1gdhLczl_XqxOztz9-GsXRk0k9o&export=download'\n",
        "output='./new.zip'\n",
        "gdown.download(dataset, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z42QAuYpIDJd"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('./new.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIE6oAQRNuPp",
        "outputId": "9b9544a0-390b-4893-a227-50aedaeb0822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DL_Project/dataset2/train/Bean\n",
            "/content/drive/MyDrive/DL_Project/dataset2/train/Capsicum\n",
            "/content/drive/MyDrive/DL_Project/dataset2/train/Papaya\n",
            "/content/drive/MyDrive/DL_Project/dataset2/test/Bean\n",
            "/content/drive/MyDrive/DL_Project/dataset2/test/Capsicum\n",
            "/content/drive/MyDrive/DL_Project/dataset2/test/Papaya\n",
            "/content/drive/MyDrive/DL_Project/dataset2/validation/Capsicum\n",
            "/content/drive/MyDrive/DL_Project/dataset2/validation/Bean\n",
            "/content/drive/MyDrive/DL_Project/dataset2/validation/Papaya\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "#SCRIPT3 FIND AND REMOVE CORRUPTED IMAGES\n",
        "import os\n",
        "from os import listdir\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "categ = ['train','test','validation']\n",
        "dataset = '/content/drive/MyDrive/DL_Project/dataset2'\n",
        "bad_file_list=[]\n",
        "bad_count=0\n",
        "\n",
        "for cat in categ:\n",
        "  img_path = os.path.join(dataset, cat)\n",
        "  for foldername in os.listdir(img_path):\n",
        "    sign_path = os.path.join(img_path, foldername)\n",
        "    print(sign_path)\n",
        "    for sign in listdir(sign_path):\n",
        "      if sign.endswith('.jpg'):\n",
        "        try:\n",
        "          Image.open(os.path.join(sign_path, sign)).load() # open the image file\n",
        "           # verify that it is, in fact an image\n",
        "        except:\n",
        "          bad_file_list.append(os.path.join(sign_path, sign))\n",
        "          bad_count +=1\n",
        "print(bad_file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLkaq5gqOCG7"
      },
      "outputs": [],
      "source": [
        "#SCRIPT4  CREATE DICTIONARY WITH CANDIDATE HYPERPARAMS\n",
        "\n",
        "models = []\n",
        "one = {\"kernel_num\": 64, \"kernel_size\": 3, \"fc_size\": 1024, \"conv_layer\":4}\n",
        "two = {\"kernel_num\": 64, \"kernel_size\": 3, \"fc_size\": 1024, \"conv_layer\":3}\n",
        "three = {\"kernel_num\": 32, \"kernel_size\": 3, \"fc_size\": 768, \"conv_layer\":3}\n",
        "four = {\"kernel_num\": 32, \"kernel_size\": 3, \"fc_size\": 1024, \"conv_layer\":4}\n",
        "five = {\"kernel_num\": 64, \"kernel_size\": 4, \"fc_size\": 512, \"conv_layer\":4}\n",
        "six = {\"kernel_num\": 32, \"kernel_size\": 4, \"fc_size\": 768, \"conv_layer\":2}\n",
        "\n",
        "models.append(one)\n",
        "models.append(two)\n",
        "models.append(three)\n",
        "models.append(four)\n",
        "models.append(five)\n",
        "models.append(six)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk4TEZhtOGyl"
      },
      "outputs": [],
      "source": [
        "#SCRIPT5 DEFINE SAVE/LOAD STATE\n",
        "import os\n",
        "import pickle\n",
        "def save_state(mod,epochs,model_type):\n",
        "    with open('state.state', 'wb') as cfg:\n",
        "      pickle.dump(mod, cfg)\n",
        "      pickle.dump(epochs,cfg)\n",
        "      pickle.dump(model_type,cfg)\n",
        "\n",
        "def load_state():\n",
        "    with open('state.state','rb') as cffile: \n",
        "      model2 = pickle.load(cffile)\n",
        "      epochs2 = pickle.load(cffile)\n",
        "      model_type = pickle.load(cffile)\n",
        "    return model2, epochs2, model_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbKf117POMUp"
      },
      "outputs": [],
      "source": [
        "#create model function\n",
        "from tensorflow.keras.layers import Input, Conv2D , Dropout, MaxPool2D, Flatten, Dense \n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "\n",
        "def create_model(ker_num,ker_size,fc_size,conv_layer_num):\n",
        "  \n",
        "  input = Input(shape =(150,150,3))\n",
        "  weight_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=1234)\n",
        "  bias_initializer=tf.keras.initializers.Zeros()\n",
        "\n",
        "  if conv_layer_num == 2:\n",
        "    x = Conv2D (filters =ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(input)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "    x = Conv2D (filters =2*ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "  elif conv_layer_num == 3:\n",
        "\n",
        "    x = Conv2D (filters =ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(input)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "    x = Conv2D (filters =2*ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "    x = Conv2D (filters =4*ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "  else:\n",
        "    x = Conv2D (filters =ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(input)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "    x = Conv2D (filters =2*ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "    x = Conv2D (filters =4*ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "    x = Conv2D (filters =8*ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "\n",
        "  x = Flatten()(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(units = fc_size, activation ='relu', kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "  output = Dense(units = 3, activation ='softmax')(x)\n",
        "\n",
        "  model = Model (inputs=input, outputs =output)\n",
        "  model.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  return model\n",
        "\n",
        "#model = create_model(8,3,0)\n",
        "#print(model.loss)\n",
        "#print(model.optimizer)\n",
        "#print(model.metrics_names)\n",
        "#print(model.metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VcULBP6OaIK"
      },
      "outputs": [],
      "source": [
        "#FUNCTION FOR GETTING DATASETS\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def get_datasets():\n",
        "#appply data augmentation on TRAIN DATA ONLY!!\n",
        "  train_datagen = ImageDataGenerator( rescale = 1.0/255.,rotation_range=40,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.2,horizontal_flip=True) \n",
        "\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "    directory=r\"./train/\",\n",
        "    target_size=(150, 150),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=16,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    seed=42)\n",
        "\n",
        "  val_datagen = ImageDataGenerator( rescale = 1.0/255.)\n",
        "\n",
        "  val_generator = val_datagen.flow_from_directory(\n",
        "    directory=r\"./validation/\",\n",
        "    target_size=(150, 150),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=16,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    seed=42)\n",
        "\n",
        "  test_datagen = ImageDataGenerator( rescale = 1.0/255.)\n",
        "\n",
        "  test_generator = test_datagen.flow_from_directory(\n",
        "    directory=r\"./test/\",\n",
        "    target_size=(150, 150),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=16,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    seed=42)\n",
        "  \n",
        "  return train_generator, val_generator, test_generator\n",
        "  one, two, three = get_datasets()\n",
        "  print(one.class_indices)\n",
        "  print(two.class_indices())\n",
        "  print(three.class_indices())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uvfs8Kaw-_c",
        "outputId": "756cfdf9-245b-4eca-c3bd-578258ef5705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3000 images belonging to 3 classes.\n",
            "Found 600 images belonging to 3 classes.\n",
            "Found 600 images belonging to 3 classes.\n",
            "{'Bean': 0, 'Capsicum': 1, 'Papaya': 2}\n",
            "{'Bean': 0, 'Capsicum': 1, 'Papaya': 2}\n",
            "{'Bean': 0, 'Capsicum': 1, 'Papaya': 2}\n"
          ]
        }
      ],
      "source": [
        "train_generator, val_generator, test_generator = get_datasets() \n",
        "print(test_generator.class_indices)\n",
        "print(val_generator.class_indices)\n",
        "print(train_generator.class_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDMNW1CYO3y5"
      },
      "outputs": [],
      "source": [
        "#CALLBACK\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from timeit import default_timer as timer\n",
        "stop_flag = False\n",
        "\n",
        "\n",
        "class newcb(keras.callbacks.Callback):\n",
        "  def __init__(self, start):\n",
        "    self.start = start\n",
        "    print(self.start,'\\nINIT TIME\\n')\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "      end = timer()\n",
        "      print('\\nCALLBACK time ', end,'\\n')\n",
        "      if end-self.start > 80000:\n",
        "        print('\\nTERMINATING TRAIN DUE TO EXCEEDING 5 HOURS\\n')\n",
        "        print(current_arch,' SAVING ARCH\\n')\n",
        "        save_state(self.model,epoch,current_arch)\n",
        "        self.model.stop_training = True\n",
        "        global stop_flag\n",
        "        stop_flag = True\n",
        "        print(stop_flag,' STOP FLAG CALLBACK\\n')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM-nZQLKPXwu"
      },
      "outputs": [],
      "source": [
        "#MAIN TRAINNING SCRIPT\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import time\n",
        "\n",
        "stop_flag = False\n",
        "#MARK START TIME\n",
        "start = timer()\n",
        "\n",
        "#Initialize only for first run\n",
        "epochs = 50\n",
        "epochs_done = 0\n",
        "current_arch = 0\n",
        "current_model = None\n",
        "\n",
        "#IF CONTINUING LOAD STATE\n",
        "if os.path.exists('state.state'):\n",
        "  print('\\nINIT MODEL FROM SAVED STATE \\n')\n",
        "  current_model, epochs_done, current_arch = load_state()\n",
        "  print(current_model.loss)\n",
        "  print(current_model.optimizer)\n",
        "  print(current_model.metrics_names)\n",
        "  print(current_model.metrics)\n",
        "#ELSE CREATE A MODEL  \n",
        "else:\n",
        "  print('\\nCREATING MODEL\\n')\n",
        "#HYPERPARAMS FOR THIS RUN\n",
        "  model_params = models[current_arch]\n",
        "  kernel_num = model_params['kernel_num']\n",
        "  kernel_size = model_params['kernel_size']\n",
        "  fc_size = model_params['fc_size']\n",
        "  conv_num = model_params['conv_layer']\n",
        "  current_model = create_model(ker_num=kernel_num,ker_size=kernel_size,fc_size=fc_size,conv_layer_num=conv_num)\n",
        "  print(current_model.summary())  \n",
        "\n",
        "train_generator, val_generator, test_generator = get_datasets() \n",
        "\n",
        "#EARLY STOP CALLBACK\n",
        "callbackES = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10,restore_best_weights=True)\n",
        "\n",
        "#CALL BACK FOR SAVING AT 5 HOURS\n",
        "stop_cb = newcb(start=start)\n",
        "\n",
        "#ITERATE THROUGH CANDIDATE MODELS\n",
        "for arch in range(current_arch,len(models)):\n",
        "  current_arch = arch\n",
        "  print('RUNNING MODEL ',current_arch)\n",
        "  if stop_flag == True:\n",
        "    print('BREAKING FROM LOOP BECAUSE CALLBACK STOPPED TRAINNNG')\n",
        "    break\n",
        "#ITEREATE THROUGH EPOCHS  \n",
        "  history=current_model.fit(train_generator,validation_data = val_generator,epochs = epochs-epochs_done, verbose = 1,shuffle = True,callbacks=[stop_cb,callbackES])\n",
        "\n",
        "  fig, ax = plt.subplots(2)\n",
        "\n",
        "  ax[0].set_ylabel('Loss')\n",
        "  ax[1].set_ylabel('Accuracy')\n",
        "\n",
        "  ax[0].plot(history.history['loss'], color='red') \n",
        "  ax[0].plot(history.history['val_loss'], color='blue') \n",
        "\n",
        "  ax[1].plot(history.history['accuracy'], color='red') \n",
        "  ax[1].plot(history.history['val_accuracy'], color='blue') \n",
        "\n",
        "  filename2 = 'train_history'+str(arch)+'.png'\n",
        "  fig.savefig(filename2)\n",
        "  plt.show()\n",
        "\n",
        "  print(stop_flag,' AFTER FIT FLAG IS')\n",
        "  file_name = 'model_' + str(arch) + '.model'\n",
        "  \n",
        "  if stop_flag == False:\n",
        "   \n",
        "   with open(file_name, 'wb') as cfg:\n",
        "      pickle.dump(current_model,cfg)\n",
        "\n",
        "   model_params = models[current_arch+1]\n",
        "   kernel_num = model_params['kernel_num']\n",
        "   kernel_size = model_params['kernel_size']\n",
        "   fc_size = model_params['fc_size']\n",
        "   conv_num = model_params['conv_layer']\n",
        "   current_model = create_model(ker_num=kernel_num,ker_size=kernel_size,fc_size=fc_size,conv_layer_num=conv_num)\n",
        "   print(current_model.summary())  \n",
        "   epochs_done = 0   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79NUkdTD_MDF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSKEc5r4-5DQ"
      },
      "outputs": [],
      "source": [
        "#SCRIPT 10 EVALUATION\n",
        "from tensorflow.keras import Model\n",
        "import pickle\n",
        "\n",
        "models = ['model_0.model','model_1.model','model_2.model','model_3.model','model_4.model','model_5.model']\n",
        "train_generator, val_generator, test_generator = get_datasets() \n",
        "\n",
        "for i in models:\n",
        " with open(i,'rb') as cffile: \n",
        "  model = pickle.load(cffile)\n",
        "  model.summary()\n",
        "\n",
        " result = model.evaluate(test_generator)\n",
        " print('For model ',i,' result = ',result)  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Deep_Learning_ds2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
