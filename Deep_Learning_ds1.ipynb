{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEcSzJX6hND6",
        "outputId": "5c68962b-b12f-4fc4-dff9-2d22694c3058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2AOuehOhXDq",
        "outputId": "bd7cad1d-53df-4aa3-e12a-12bf0d35ebfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_0.model  new.zip\ttest_dataset  train_dataset  val_dataset\n"
          ]
        }
      ],
      "source": [
        "#import os\n",
        "#os.chdir(\"/content/drive/MyDrive/DL_Project/complete_ds\")\n",
        "#!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oFQ7_P6KWJQ"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "dataset = 'https://drive.google.com/uc?id=1n2Znas2BSAl6FGeSBW6X5xRC6we4jWg7&export=download'\n",
        "output='./new.zip'\n",
        "gdown.download(dataset, output, quiet=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQhH7bXUL-O-"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('./new.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOvkJvetheiD"
      },
      "outputs": [],
      "source": [
        "#SCRIPT3 FIND AND REMOVE CORRUPTED IMAGES\n",
        "import os\n",
        "from os import listdir\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "categ = ['train','test','validation']\n",
        "dataset = '/content/drive/MyDrive/DL_Project/complete_ds'\n",
        "bad_file_list=[]\n",
        "bad_count=0\n",
        "\n",
        "for cat in categ:\n",
        "  img_path = os.path.join(dataset, cat)\n",
        "  for foldername in os.listdir(img_path):\n",
        "    sign_path = os.path.join(img_path, foldername)\n",
        "    print(sign_path)\n",
        "    for sign in listdir(sign_path):\n",
        "      if sign.endswith('.jpg'):\n",
        "        try:\n",
        "          Image.open(os.path.join(sign_path, sign)).load() # open the image file\n",
        "           # verify that it is, in fact an image\n",
        "        except:\n",
        "          bad_file_list.append(os.path.join(sign_path, sign))\n",
        "          bad_count +=1\n",
        "print(bad_file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOtrfk5YhpCi"
      },
      "outputs": [],
      "source": [
        "#SCRIPT4  CREATE DICTIONARY WITH CANDIDATE HYPERPARAMS\n",
        "\n",
        "models = []\n",
        "one = {\"kernel_num\": 64, \"kernel_size\": 3, \"fc_size\": 1024, \"conv_layer\":4}\n",
        "two = {\"kernel_num\": 64, \"kernel_size\": 3, \"fc_size\": 1024, \"conv_layer\":3}\n",
        "three = {\"kernel_num\": 32, \"kernel_size\": 3, \"fc_size\": 768, \"conv_layer\":3}\n",
        "four = {\"kernel_num\": 32, \"kernel_size\": 3, \"fc_size\": 1024, \"conv_layer\":4}\n",
        "five = {\"kernel_num\": 64, \"kernel_size\": 4, \"fc_size\": 512, \"conv_layer\":4}\n",
        "six = {\"kernel_num\": 32, \"kernel_size\": 4, \"fc_size\": 768, \"conv_layer\":2}\n",
        "\n",
        "models.append(one)\n",
        "models.append(two)\n",
        "models.append(three)\n",
        "models.append(four)\n",
        "models.append(five)\n",
        "models.append(six)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_Ehzjqfhtaq"
      },
      "outputs": [],
      "source": [
        "#SCRIPT5 DEFINE SAVE/LOAD STATE\n",
        "import os\n",
        "import pickle\n",
        "def save_state(mod,epochs,model_type):\n",
        "    with open('state.state', 'wb') as cfg:\n",
        "      pickle.dump(mod, cfg)\n",
        "      pickle.dump(epochs,cfg)\n",
        "      pickle.dump(model_type,cfg)\n",
        "\n",
        "def load_state():\n",
        "    with open('state.state','rb') as cffile: \n",
        "      model2 = pickle.load(cffile)\n",
        "      epochs2 = pickle.load(cffile)\n",
        "      model_type = pickle.load(cffile)\n",
        "    return model2, epochs2, model_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au5uny7ehzAP"
      },
      "outputs": [],
      "source": [
        "#create model function\n",
        "from tensorflow.keras.layers import Input, Conv2D , Dropout, MaxPool2D, Flatten, Dense \n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "\n",
        "def create_model(ker_num,ker_size,fc_size,conv_layer_num):\n",
        "  \n",
        "  input = Input(shape =(150,150,3))\n",
        "  weight_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=1234)\n",
        "  bias_initializer=tf.keras.initializers.Zeros()\n",
        "\n",
        "  if conv_layer_num == 2:\n",
        "    x = Conv2D (filters =ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(input)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "    x = Conv2D (filters =2*ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "  elif conv_layer_num == 3:\n",
        "\n",
        "    x = Conv2D (filters =ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(input)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "    x = Conv2D (filters =2*ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "    x = Conv2D (filters =4*ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "  else:\n",
        "    x = Conv2D (filters =ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(input)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "    x = Conv2D (filters =2*ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "    x = Conv2D (filters =4*ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "    x = Conv2D (filters =8*ker_num, kernel_size =ker_size, padding ='same', activation='relu',kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
        "\n",
        "\n",
        "  x = Flatten()(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(units = fc_size, activation ='relu', kernel_initializer=weight_initializer,kernel_regularizer=l2(0.00005),bias_initializer=bias_initializer)(x)\n",
        "  output = Dense(units = 9, activation ='softmax')(x)\n",
        "\n",
        "  model = Model (inputs=input, outputs =output)\n",
        "  model.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7IhWw7Ch4_G"
      },
      "outputs": [],
      "source": [
        "#FUNCTION FOR GETTING DATASETS\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def get_datasets():\n",
        "#appply data augmentation on TRAIN DATA ONLY!!\n",
        "  train_datagen = ImageDataGenerator( rescale = 1.0/255.,rotation_range=40,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.2,horizontal_flip=True) \n",
        "\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "    directory=r\"./train_dataset/\",\n",
        "    target_size=(150, 150),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=16,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    seed=42)\n",
        "\n",
        "  val_datagen = ImageDataGenerator( rescale = 1.0/255.)\n",
        "\n",
        "  val_generator = val_datagen.flow_from_directory(\n",
        "    directory=r\"./val_dataset/\",\n",
        "    target_size=(150, 150),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=16,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    seed=42)\n",
        "\n",
        "  test_datagen = ImageDataGenerator( rescale = 1.0/255.)\n",
        "\n",
        "  test_generator = test_datagen.flow_from_directory(\n",
        "    directory=r\"./test_dataset/\",\n",
        "    target_size=(150, 150),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=16,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    seed=42)\n",
        "  \n",
        "  return train_generator, val_generator, test_generator\n",
        "  one, two, three = get_datasets()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj79KpQnh_Vd"
      },
      "outputs": [],
      "source": [
        "#CALLBACK\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from timeit import default_timer as timer\n",
        "stop_flag = False\n",
        "\n",
        "\n",
        "class newcb(keras.callbacks.Callback):\n",
        "  def __init__(self, start):\n",
        "    self.start = start\n",
        "    print(self.start,'\\nINIT TIME\\n')\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "      end = timer()\n",
        "      print('\\nCALLBACK time ', end,'\\n')\n",
        "      if end-self.start > 80000:\n",
        "        print('\\nTERMINATING TRAIN DUE TO EXCEEDING 5 HOURS\\n')\n",
        "        print(current_arch,' SAVING ARCH\\n')\n",
        "        save_state(self.model,epoch,current_arch)\n",
        "        self.model.stop_training = True\n",
        "        global stop_flag\n",
        "        stop_flag = True\n",
        "        print(stop_flag,' STOP FLAG CALLBACK\\n')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxrVYmYeiFxa",
        "outputId": "6b3d8a44-22b3-4830-bf91-e48389f0a3e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CREATING MODEL\n",
            "\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 150, 150, 64)      1792      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 75, 75, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 75, 75, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 38, 38, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 38, 38, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 19, 19, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 19, 19, 512)       1180160   \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 10, 10, 512)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 51200)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 51200)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              52429824  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 9)                 9225      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53,990,025\n",
            "Trainable params: 53,990,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Found 2777 images belonging to 9 classes.\n",
            "Found 450 images belonging to 9 classes.\n",
            "Found 450 images belonging to 9 classes.\n",
            "{'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7, '9': 8}\n",
            "{'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7, '9': 8}\n",
            "{'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7, '9': 8}\n",
            "178.661852825 \n",
            "INIT TIME\n",
            "\n",
            "RUNNING MODEL  0\n",
            "Epoch 1/50\n",
            "174/174 [==============================] - ETA: 0s - loss: 3972293.0000 - accuracy: 0.1981\n",
            "CALLBACK time  1137.874096297 \n",
            "\n",
            "174/174 [==============================] - 945s 5s/step - loss: 3972293.0000 - accuracy: 0.1981 - val_loss: 1375319.5000 - val_accuracy: 0.1200\n",
            "Epoch 2/50\n",
            " 37/174 [=====>........................] - ETA: 7:58 - loss: 1148385.5000 - accuracy: 0.1983"
          ]
        }
      ],
      "source": [
        "#MAIN TRAINNING SCRIPT\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import time\n",
        "\n",
        "stop_flag = False\n",
        "#MARK START TIME\n",
        "start = timer()\n",
        "\n",
        "#Initialize only for first run\n",
        "epochs = 50\n",
        "epochs_done = 0\n",
        "current_arch = 0\n",
        "current_model = None\n",
        "\n",
        "#IF CONTINUING LOAD STATE\n",
        "if os.path.exists('state.state'):\n",
        "  print('\\nINIT MODEL FROM SAVED STATE \\n')\n",
        "  current_model, epochs_done, current_arch = load_state()\n",
        "  print(current_model.loss)\n",
        "  print(current_model.optimizer)\n",
        "  print(current_model.metrics_names)\n",
        "  print(current_model.metrics)\n",
        "#ELSE CREATE A MODEL  \n",
        "else:\n",
        "  print('\\nCREATING MODEL\\n')\n",
        "#HYPERPARAMS FOR THIS RUN\n",
        "  model_params = models[current_arch]\n",
        "  kernel_num = model_params['kernel_num']\n",
        "  kernel_size = model_params['kernel_size']\n",
        "  fc_size = model_params['fc_size']\n",
        "  conv_num = model_params['conv_layer']\n",
        "  current_model = create_model(ker_num=kernel_num,ker_size=kernel_size,fc_size=fc_size,conv_layer_num=conv_num)\n",
        "  print(current_model.summary())  \n",
        "\n",
        "train_generator, val_generator, test_generator = get_datasets() \n",
        "print(train_generator.class_indices)\n",
        "print(val_generator.class_indices)\n",
        "print(test_generator.class_indices)\n",
        "\n",
        "#EARLY STOP CALLBACK\n",
        "callbackES = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10,restore_best_weights=True)\n",
        "\n",
        "#CALL BACK FOR SAVING AT 5 HOURS\n",
        "stop_cb = newcb(start=start)\n",
        "\n",
        "#ITERATE THROUGH CANDIDATE MODELS\n",
        "for arch in range(current_arch,len(models)):\n",
        "  current_arch = arch\n",
        "  print('RUNNING MODEL ',current_arch)\n",
        "  if stop_flag == True:\n",
        "    print('BREAKING FROM LOOP BECAUSE CALLBACK STOPPED TRAINNNG')\n",
        "    break\n",
        "#ITEREATE THROUGH EPOCHS  \n",
        "  history=current_model.fit(train_generator,validation_data = val_generator,epochs = epochs-epochs_done, verbose = 1,shuffle = True,callbacks=[stop_cb,callbackES])\n",
        "\n",
        "  fig, ax = plt.subplots(2)\n",
        "\n",
        "  ax[0].set_ylabel('Loss')\n",
        "  ax[1].set_ylabel('Accuracy')\n",
        "\n",
        "  ax[0].plot(history.history['loss'], color='red') \n",
        "  ax[0].plot(history.history['val_loss'], color='blue') \n",
        "\n",
        "  ax[1].plot(history.history['accuracy'], color='red') \n",
        "  ax[1].plot(history.history['val_accuracy'], color='blue') \n",
        "\n",
        "  filename2 = 'train_history'+str(arch)+'.png'\n",
        "  fig.savefig(filename2)\n",
        "  plt.show()\n",
        "\n",
        "  print(stop_flag,' AFTER FIT FLAG IS')\n",
        "  file_name = 'model_' + str(arch) + '.model'\n",
        "  \n",
        "  if stop_flag == False:\n",
        "   \n",
        "   with open(file_name, 'wb') as cfg:\n",
        "      pickle.dump(current_model,cfg)\n",
        "\n",
        "   model_params = models[current_arch+1]\n",
        "   kernel_num = model_params['kernel_num']\n",
        "   kernel_size = model_params['kernel_size']\n",
        "   fc_size = model_params['fc_size']\n",
        "   conv_num = model_params['conv_layer']\n",
        "   current_model = create_model(ker_num=kernel_num,ker_size=kernel_size,fc_size=fc_size,conv_layer_num=conv_num)\n",
        "   print(current_model.summary())  \n",
        "   epochs_done = 0   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F4ZevaTaSOx"
      },
      "outputs": [],
      "source": [
        "#SCRIPT 10 EVALUATION\n",
        "from tensorflow.keras import Model\n",
        "import pickle\n",
        "\n",
        "models = ['model_0.model','model_1.model','model_2.model','model_3.model','model_4.model','model_5.model']\n",
        "train_generator, val_generator, test_generator = get_datasets() \n",
        "\n",
        "for i in models:\n",
        " with open(i,'rb') as cffile: \n",
        "  model = pickle.load(cffile)\n",
        "  model.summary()\n",
        "\n",
        " result = model.evaluate(test_generator)\n",
        " print('For model ',i,' result = ',result) "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Deep_Learning_ds1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
